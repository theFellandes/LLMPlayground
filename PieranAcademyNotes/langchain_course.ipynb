{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3414b840d7106ef8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Models Inputs and Outputs\n",
    "\n",
    "### Large Language Models\n",
    "\n",
    "LLM -> Text completion model: returns the most likely text to continue\n",
    "\n",
    "Chat -> Converses with back and forth messages, can also have a \"system\" prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5deeae67d3dd8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Chat Models\n",
    "\n",
    "Chat Models have a series of messages, just like a chat text thread, except one side of the conversation is an AI LLM. \n",
    "\n",
    "Langchain creates 3 schema objects for this:\n",
    "\tSystemMessage: General system tone or personality\n",
    "\tHumanMessage: Human request or reply\n",
    "\tAIMessage: AI's reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847948a1253fde97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:22:50.048495300Z",
     "start_time": "2024-12-09T12:22:49.773290900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[1;32m----> 2\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(openai_api_key\u001b[38;5;241m=\u001b[39m\u001b[43mapi_key\u001b[49m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, HumanMessage, SystemMessage\n\u001b[0;32m      5\u001b[0m result \u001b[38;5;241m=\u001b[39m chat([SystemMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou are a very rude teenager who only wants to party and not want to answer the question.\u001b[39m\u001b[38;5;124m'\u001b[39m), HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTell me a fact about Pluto\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=api_key)\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "result = chat([SystemMessage(content='You are a very rude teenager who only wants to party and not want to answer the question.'), HumanMessage(content='Tell me a fact about Pluto')])\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2f32434e438725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:19:24.001866Z",
     "start_time": "2024-12-09T12:19:23.987180500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      2\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatOpenAI(openai_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, HumanMessage, SystemMessage\n",
      "File \u001b[1;32mD:\\Programming\\Python\\LLMPlayground\\.venv\\Lib\\site-packages\\langchain\\chat_models\\__init__.py:29\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chat_models\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# If not in interactive env, raise warning.\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_interactive_env():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(openai_api_key=api_key)\n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "# Passing lists to get multiple responses.\n",
    "result = chat.generate([SystemMessage(content='You are a very rude teenager who only wants to party and not want to answer the question.'), HumanMessage(content='Tell me a fact about Pluto')],\n",
    "[SystemMessage(content='You are helpful asssistant'), HumanMessage(content='Tell me a fact about Pluto')])\n",
    "print(result.generations[0][0].text) # Returns the response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c20f3dc9b5565f9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b0162d3a780a2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:24:16.364574800Z",
     "start_time": "2024-12-09T12:24:16.178928800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a61fdcfdce9c828",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:24:27.849219500Z",
     "start_time": "2024-12-09T12:24:27.841897600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# LangChain's Example:\n",
    "from langchain.cache import InMemoryCache\n",
    "from langchain.globals import set_llm_cache\n",
    "\n",
    "set_llm_cache(InMemoryCache())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49f3f7f59e8aa3",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Prompt Templates\n",
    "\n",
    "Templates allow us to easily configure and modify our input prompts to LLM calls. Templates offer a more systematic approach to passing in variables to prompts for models instead of using f-string literals or .format() calls, the PromptTemplate converts these into function parameter names that we can pass in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f22ee2adb9c4721f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:48:07.485244100Z",
     "start_time": "2024-12-09T12:48:07.476903100Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "no_prompt = PromptTemplate(input_variables=[], template=\"This is a single prompt template\")\n",
    "input_prompt = PromptTemplate(input_variables=[\"topic\"], template=\"This is a prompt template with input variables {topic}\")\n",
    "input_prompt.format(topic=\"Pluto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c7cf3b86fdf5a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T13:06:44.865422Z",
     "start_time": "2024-12-09T13:06:44.857792400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are an AI recipe assistant that specializes in vegan recipes.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Vegan Chocolate Cake', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "system_template = \"You are an AI recipe assistant that specializes in {dietary_preference} recipes.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "human_template = \"{recipe}\"\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_template])\n",
    "chat_prompt.input_variables\n",
    "chat_prompt.format_prompt(dietary_preference=\"vegan\", recipe=\"Vegan Chocolate Cake\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaefb49d80310ca",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Few Shot Prompt Templates\n",
    "\n",
    "Sometimes it's easier to give the LLM a few examples of input/output pairs before sending your main request. This allows the LLM to \"learn\" the pattern you are looking for and may lead to better results. It should be noted that there is currently no consensus on best practices but LangChain recommends building a history of Human and AI message inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e154e4bfa93514",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
    "legal_text = \"You are a lawyer who specializes in {law_specialty} law.\"\n",
    "example_input_one = HumanMessagePromptTemplate.from_template(legal_text)\n",
    "\n",
    "plain_text = \"The rules in this agreement can be separated.\"\n",
    "example_output_one = AIMessagePromptTemplate.from_template(plain_text)\n",
    "\n",
    "human_template = \"{legal_text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([example_input_one, example_output_one, human_message_prompt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
